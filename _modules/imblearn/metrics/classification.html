

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>imblearn.metrics.classification &mdash; imbalanced-learn 0.3.0.dev0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../../_static/css/imbalanced-learn.css" type="text/css" />
  

  
    <link rel="stylesheet" href="../../../_static/gallery.css" type="text/css" />
  

  
        <link rel="author" title="About these documents"
              href="../../../about.html"/>
        <link rel="index" title="Index"
              href="../../../genindex.html"/>
        <link rel="search" title="Search" href="../../../search.html"/>
    <link rel="top" title="imbalanced-learn 0.3.0.dev0 documentation" href="../../../index.html"/>
        <link rel="up" title="Module code" href="../../index.html"/> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../index.html" class="icon icon-home"> imbalanced-learn
          

          
          </a>

          
            
            
              <div class="version">
                0.3.0.dev0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../install.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../support.html">Support</a></li>
</ul>
<p class="caption"><span class="caption-text">API Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api.html"><cite>imbalanced-learn</cite> API</a></li>
</ul>
<p class="caption"><span class="caption-text">Tutorial - Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../auto_examples/index.html">General examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../auto_examples/index.html#examples-based-on-real-world-datasets">Examples based on real world datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../auto_examples/index.html#dataset-examples">Dataset examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../auto_examples/index.html#evaluation-examples">Evaluation examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../auto_examples/index.html#model-selection">Model Selection</a></li>
</ul>
<p class="caption"><span class="caption-text">Addtional information</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../whats_new.html">Release history</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../todo.html">To Do list</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../about.html">About us</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">imbalanced-learn</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>imblearn.metrics.classification</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for imblearn.metrics.classification</h1><div class="highlight"><pre>
<span></span><span class="c1"># coding: utf-8</span>
<span class="sd">&quot;&quot;&quot;Metrics to assess performance on classification task given class prediction</span>

<span class="sd">Functions named as ``*_score`` return a scalar value to maximize: the higher</span>
<span class="sd">the better</span>

<span class="sd">Function named as ``*_error`` or ``*_loss`` return a scalar value to minimize:</span>
<span class="sd">the lower the better</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># Authors: Guillaume Lemaitre &lt;g.lemaitre58@gmail.com&gt;</span>
<span class="c1">#          Dariusz Brzezinski</span>
<span class="c1"># License: MIT</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">functools</span>

<span class="kn">from</span> <span class="nn">inspect</span> <span class="k">import</span> <span class="n">getcallargs</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics.classification</span> <span class="k">import</span> <span class="p">(</span><span class="n">_check_targets</span><span class="p">,</span> <span class="n">_prf_divide</span><span class="p">,</span>
                                            <span class="n">precision_recall_fscore_support</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.multiclass</span> <span class="k">import</span> <span class="n">unique_labels</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">inspect</span> <span class="k">import</span> <span class="n">signature</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">sklearn.externals.funcsigs</span> <span class="k">import</span> <span class="n">signature</span>


<span class="n">LOGGER</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<div class="viewcode-block" id="sensitivity_specificity_support"><a class="viewcode-back" href="../../../generated/imblearn.metrics.sensitivity_specificity_support.html#imblearn.metrics.sensitivity_specificity_support">[docs]</a><span class="k">def</span> <span class="nf">sensitivity_specificity_support</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span>
                                    <span class="n">y_pred</span><span class="p">,</span>
                                    <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                    <span class="n">pos_label</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                    <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                    <span class="n">warn_for</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;sensitivity&#39;</span><span class="p">,</span> <span class="s1">&#39;specificity&#39;</span><span class="p">),</span>
                                    <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute sensitivity, specificity, and support for each class</span>

<span class="sd">    The sensitivity is the ratio ``tp / (tp + fn)`` where ``tp`` is the number</span>
<span class="sd">    of true positives and ``fn`` the number of false negatives. The sensitivity</span>
<span class="sd">    quantifies the ability to avoid false negatives_[1].</span>

<span class="sd">    The specificity is the ratio ``tn / (tn + fp)`` where ``tn`` is the number</span>
<span class="sd">    of true negatives and ``fn`` the number of false negatives. The specificity</span>
<span class="sd">    quantifies the ability to avoid false positives_[1].</span>

<span class="sd">    The support is the number of occurrences of each class in ``y_true``.</span>

<span class="sd">    If ``pos_label is None`` and in binary classification, this function</span>
<span class="sd">    returns the average sensitivity and specificity if ``average``</span>
<span class="sd">    is one of ``&#39;weighted&#39;``.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : ndarray, shape (n_samples, )</span>
<span class="sd">        Ground truth (correct) target values.</span>

<span class="sd">    y_pred : ndarray, shape (n_samples, )</span>
<span class="sd">        Estimated targets as returned by a classifier.</span>

<span class="sd">    labels : list, optional</span>
<span class="sd">        The set of labels to include when ``average != &#39;binary&#39;``, and their</span>
<span class="sd">        order if ``average is None``. Labels present in the data can be</span>
<span class="sd">        excluded, for example to calculate a multiclass average ignoring a</span>
<span class="sd">        majority negative class, while labels not present in the data will</span>
<span class="sd">        result in 0 components in a macro average. For multilabel targets,</span>
<span class="sd">        labels are column indices. By default, all labels in ``y_true`` and</span>
<span class="sd">        ``y_pred`` are used in sorted order.</span>

<span class="sd">    pos_label : str or int, optional (default=1)</span>
<span class="sd">        The class to report if ``average=&#39;binary&#39;`` and the data is binary.</span>
<span class="sd">        If the data are multiclass, this will be ignored;</span>
<span class="sd">        setting ``labels=[pos_label]`` and ``average != &#39;binary&#39;`` will report</span>
<span class="sd">        scores for that label only.</span>

<span class="sd">    average : str or None, optional (default=None)</span>
<span class="sd">        If ``None``, the scores for each class are returned. Otherwise, this</span>
<span class="sd">        determines the type of averaging performed on the data:</span>

<span class="sd">        ``&#39;binary&#39;``:</span>
<span class="sd">            Only report results for the class specified by ``pos_label``.</span>
<span class="sd">            This is applicable only if targets (``y_{true,pred}``) are binary.</span>
<span class="sd">        ``&#39;micro&#39;``:</span>
<span class="sd">            Calculate metrics globally by counting the total true positives,</span>
<span class="sd">            false negatives and false positives.</span>
<span class="sd">        ``&#39;macro&#39;``:</span>
<span class="sd">            Calculate metrics for each label, and find their unweighted</span>
<span class="sd">            mean.  This does not take label imbalance into account.</span>
<span class="sd">        ``&#39;weighted&#39;``:</span>
<span class="sd">            Calculate metrics for each label, and find their average, weighted</span>
<span class="sd">            by support (the number of true instances for each label). This</span>
<span class="sd">            alters &#39;macro&#39; to account for label imbalance; it can result in an</span>
<span class="sd">            F-score that is not between precision and recall.</span>
<span class="sd">        ``&#39;samples&#39;``:</span>
<span class="sd">            Calculate metrics for each instance, and find their average (only</span>
<span class="sd">            meaningful for multilabel classification where this differs from</span>
<span class="sd">            :func:`accuracy_score`).</span>
<span class="sd">    warn_for : tuple or set, for internal use</span>
<span class="sd">        This determines which warnings will be made in the case that this</span>
<span class="sd">        function is being used to return only one of its metrics.</span>

<span class="sd">    sample_weight : ndarray, shape (n_samples, )</span>
<span class="sd">        Sample weights.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    sensitivity : float (if ``average`` = None) or ndarray, \</span>
<span class="sd">        shape (n_unique_labels, )</span>

<span class="sd">    specificity : float (if ``average`` = None) or ndarray, \</span>
<span class="sd">        shape (n_unique_labels, )</span>

<span class="sd">    support : int (if ``average`` = None) or ndarray, \</span>
<span class="sd">        shape (n_unique_labels, )</span>
<span class="sd">        The number of occurrences of each label in ``y_true``.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; from imblearn.metrics import sensitivity_specificity_support</span>
<span class="sd">    &gt;&gt;&gt; y_true = np.array([&#39;cat&#39;, &#39;dog&#39;, &#39;pig&#39;, &#39;cat&#39;, &#39;dog&#39;, &#39;pig&#39;])</span>
<span class="sd">    &gt;&gt;&gt; y_pred = np.array([&#39;cat&#39;, &#39;pig&#39;, &#39;dog&#39;, &#39;cat&#39;, &#39;cat&#39;, &#39;dog&#39;])</span>
<span class="sd">    &gt;&gt;&gt; sensitivity_specificity_support(y_true, y_pred, average=&#39;macro&#39;)</span>
<span class="sd">    (0.33333333333333331, 0.66666666666666663, None)</span>
<span class="sd">    &gt;&gt;&gt; sensitivity_specificity_support(y_true, y_pred, average=&#39;micro&#39;)</span>
<span class="sd">    (0.33333333333333331, 0.66666666666666663, None)</span>
<span class="sd">    &gt;&gt;&gt; sensitivity_specificity_support(y_true, y_pred, average=&#39;weighted&#39;)</span>
<span class="sd">    (0.33333333333333331, 0.66666666666666663, None)</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] `Wikipedia entry for the Sensitivity and specificity</span>
<span class="sd">           &lt;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&gt;`_</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">average_options</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;micro&#39;</span><span class="p">,</span> <span class="s1">&#39;macro&#39;</span><span class="p">,</span> <span class="s1">&#39;weighted&#39;</span><span class="p">,</span> <span class="s1">&#39;samples&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">average</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">average_options</span> <span class="ow">and</span> <span class="n">average</span> <span class="o">!=</span> <span class="s1">&#39;binary&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;average has to be one of &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">average_options</span><span class="p">))</span>

    <span class="n">y_type</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">_check_targets</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">present_labels</span> <span class="o">=</span> <span class="n">unique_labels</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">average</span> <span class="o">==</span> <span class="s1">&#39;binary&#39;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">y_type</span> <span class="o">==</span> <span class="s1">&#39;binary&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">pos_label</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">present_labels</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">present_labels</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
                    <span class="c1"># Only negative labels</span>
                    <span class="k">return</span> <span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;pos_label=</span><span class="si">%r</span><span class="s2"> is not a valid label: </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span>
                                     <span class="p">(</span><span class="n">pos_label</span><span class="p">,</span> <span class="n">present_labels</span><span class="p">))</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">pos_label</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Target is </span><span class="si">%s</span><span class="s2"> but average=&#39;binary&#39;. Please &quot;</span>
                             <span class="s2">&quot;choose another average setting.&quot;</span> <span class="o">%</span> <span class="n">y_type</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">pos_label</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Note that pos_label (set to </span><span class="si">%r</span><span class="s2">) is ignored when &quot;</span>
                      <span class="s2">&quot;average != &#39;binary&#39; (got </span><span class="si">%r</span><span class="s2">). You may use &quot;</span>
                      <span class="s2">&quot;labels=[pos_label] to specify a single positive class.&quot;</span>
                      <span class="o">%</span> <span class="p">(</span><span class="n">pos_label</span><span class="p">,</span> <span class="n">average</span><span class="p">),</span> <span class="ne">UserWarning</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">present_labels</span>
        <span class="n">n_labels</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">n_labels</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span>
            <span class="p">[</span><span class="n">labels</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">setdiff1d</span><span class="p">(</span>
                <span class="n">present_labels</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">assume_unique</span><span class="o">=</span><span class="kc">True</span><span class="p">)])</span>

    <span class="c1"># Calculate tp_sum, pred_sum, true_sum ###</span>

    <span class="k">if</span> <span class="n">y_type</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;multilabel&#39;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;imblearn does not support multilabel&#39;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">average</span> <span class="o">==</span> <span class="s1">&#39;samples&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Sample-based precision, recall, fscore is &quot;</span>
                         <span class="s2">&quot;not meaningful outside multilabel &quot;</span>
                         <span class="s2">&quot;classification. See the accuracy_score instead.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
        <span class="n">le</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
        <span class="n">y_true</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">sorted_labels</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">classes_</span>

        <span class="c1"># labels are now from 0 to len(labels) - 1 -&gt; use bincount</span>
        <span class="n">tp</span> <span class="o">=</span> <span class="n">y_true</span> <span class="o">==</span> <span class="n">y_pred</span>
        <span class="n">tp_bins</span> <span class="o">=</span> <span class="n">y_true</span><span class="p">[</span><span class="n">tp</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tp_bins_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">)[</span><span class="n">tp</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tp_bins_weights</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tp_bins</span><span class="p">):</span>
            <span class="n">tp_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span>
                <span class="n">tp_bins</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">tp_bins_weights</span><span class="p">,</span> <span class="n">minlength</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Pathological case</span>
            <span class="n">true_sum</span> <span class="o">=</span> <span class="n">pred_sum</span> <span class="o">=</span> <span class="n">tp_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred</span><span class="p">):</span>
            <span class="n">pred_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span>
                <span class="n">y_pred</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">minlength</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">):</span>
            <span class="n">true_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span>
                <span class="n">y_true</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">minlength</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span>

        <span class="c1"># Compute the true negative</span>
        <span class="n">tn_sum</span> <span class="o">=</span> <span class="n">y_true</span><span class="o">.</span><span class="n">size</span> <span class="o">-</span> <span class="p">(</span><span class="n">pred_sum</span> <span class="o">+</span> <span class="n">true_sum</span> <span class="o">-</span> <span class="n">tp_sum</span><span class="p">)</span>

        <span class="c1"># Retain only selected labels</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">searchsorted</span><span class="p">(</span><span class="n">sorted_labels</span><span class="p">,</span> <span class="n">labels</span><span class="p">[:</span><span class="n">n_labels</span><span class="p">])</span>
        <span class="n">tp_sum</span> <span class="o">=</span> <span class="n">tp_sum</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
        <span class="n">true_sum</span> <span class="o">=</span> <span class="n">true_sum</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
        <span class="n">pred_sum</span> <span class="o">=</span> <span class="n">pred_sum</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
        <span class="n">tn_sum</span> <span class="o">=</span> <span class="n">tn_sum</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">average</span> <span class="o">==</span> <span class="s1">&#39;micro&#39;</span><span class="p">:</span>
        <span class="n">tp_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">tp_sum</span><span class="o">.</span><span class="n">sum</span><span class="p">()])</span>
        <span class="n">pred_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">pred_sum</span><span class="o">.</span><span class="n">sum</span><span class="p">()])</span>
        <span class="n">true_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">true_sum</span><span class="o">.</span><span class="n">sum</span><span class="p">()])</span>
        <span class="n">tn_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">tn_sum</span><span class="o">.</span><span class="n">sum</span><span class="p">()])</span>

    <span class="c1"># Finally, we have all our sufficient statistics. Divide! #</span>

    <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">errstate</span><span class="p">(</span><span class="n">divide</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">invalid</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">):</span>
        <span class="c1"># Divide, and on zero-division, set scores to 0 and warn:</span>

        <span class="c1"># Oddly, we may get an &quot;invalid&quot; rather than a &quot;divide&quot; error</span>
        <span class="c1"># here.</span>
        <span class="n">specificity</span> <span class="o">=</span> <span class="n">_prf_divide</span><span class="p">(</span><span class="n">tn_sum</span><span class="p">,</span> <span class="n">tn_sum</span> <span class="o">+</span> <span class="n">pred_sum</span> <span class="o">-</span> <span class="n">tp_sum</span><span class="p">,</span>
                                  <span class="s1">&#39;specificity&#39;</span><span class="p">,</span> <span class="s1">&#39;predicted&#39;</span><span class="p">,</span> <span class="n">average</span><span class="p">,</span>
                                  <span class="n">warn_for</span><span class="p">)</span>
        <span class="n">sensitivity</span> <span class="o">=</span> <span class="n">_prf_divide</span><span class="p">(</span><span class="n">tp_sum</span><span class="p">,</span> <span class="n">true_sum</span><span class="p">,</span> <span class="s1">&#39;sensitivity&#39;</span><span class="p">,</span> <span class="s1">&#39;true&#39;</span><span class="p">,</span>
                                  <span class="n">average</span><span class="p">,</span> <span class="n">warn_for</span><span class="p">)</span>

    <span class="c1"># Average the results</span>

    <span class="k">if</span> <span class="n">average</span> <span class="o">==</span> <span class="s1">&#39;weighted&#39;</span><span class="p">:</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">true_sum</span>
        <span class="k">if</span> <span class="n">weights</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="n">average</span> <span class="o">==</span> <span class="s1">&#39;samples&#39;</span><span class="p">:</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">sample_weight</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">average</span> <span class="o">!=</span> <span class="s1">&#39;binary&#39;</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">specificity</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="n">specificity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">specificity</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
        <span class="n">sensitivity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">sensitivity</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
        <span class="n">true_sum</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># return no support</span>

    <span class="k">return</span> <span class="n">sensitivity</span><span class="p">,</span> <span class="n">specificity</span><span class="p">,</span> <span class="n">true_sum</span></div>


<div class="viewcode-block" id="sensitivity_score"><a class="viewcode-back" href="../../../generated/imblearn.metrics.sensitivity_score.html#imblearn.metrics.sensitivity_score">[docs]</a><span class="k">def</span> <span class="nf">sensitivity_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span>
                      <span class="n">y_pred</span><span class="p">,</span>
                      <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">pos_label</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                      <span class="n">average</span><span class="o">=</span><span class="s1">&#39;binary&#39;</span><span class="p">,</span>
                      <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the sensitivity</span>

<span class="sd">    The sensitivity is the ratio ``tp / (tp + fn)`` where ``tp`` is the number</span>
<span class="sd">    of true positives and ``fn`` the number of false negatives. The sensitivity</span>
<span class="sd">    quantifies the ability to avoid false negatives.</span>

<span class="sd">    The best value is 1 and the worst value is 0.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : ndarray, shape (n_samples, )</span>
<span class="sd">        Ground truth (correct) target values.</span>

<span class="sd">    y_pred : ndarray, shape (n_samples, )</span>
<span class="sd">        Estimated targets as returned by a classifier.</span>

<span class="sd">    labels : list, optional</span>
<span class="sd">        The set of labels to include when ``average != &#39;binary&#39;``, and their</span>
<span class="sd">        order if ``average is None``. Labels present in the data can be</span>
<span class="sd">        excluded, for example to calculate a multiclass average ignoring a</span>
<span class="sd">        majority negative class, while labels not present in the data will</span>
<span class="sd">        result in 0 components in a macro average.</span>

<span class="sd">    pos_label : str or int, optional (default=1)</span>
<span class="sd">        The class to report if ``average=&#39;binary&#39;`` and the data is binary.</span>
<span class="sd">        If the data are multiclass, this will be ignored;</span>
<span class="sd">        setting ``labels=[pos_label]`` and ``average != &#39;binary&#39;`` will report</span>
<span class="sd">        scores for that label only.</span>

<span class="sd">    average : str or None, optional (default=None)</span>
<span class="sd">        If ``None``, the scores for each class are returned. Otherwise, this</span>
<span class="sd">        determines the type of averaging performed on the data:</span>

<span class="sd">        ``&#39;binary&#39;``:</span>
<span class="sd">            Only report results for the class specified by ``pos_label``.</span>
<span class="sd">            This is applicable only if targets (``y_{true,pred}``) are binary.</span>
<span class="sd">        ``&#39;micro&#39;``:</span>
<span class="sd">            Calculate metrics globally by counting the total true positives,</span>
<span class="sd">            false negatives and false positives.</span>
<span class="sd">        ``&#39;macro&#39;``:</span>
<span class="sd">            Calculate metrics for each label, and find their unweighted</span>
<span class="sd">            mean.  This does not take label imbalance into account.</span>
<span class="sd">        ``&#39;weighted&#39;``:</span>
<span class="sd">            Calculate metrics for each label, and find their average, weighted</span>
<span class="sd">            by support (the number of true instances for each label). This</span>
<span class="sd">            alters &#39;macro&#39; to account for label imbalance; it can result in an</span>
<span class="sd">            F-score that is not between precision and recall.</span>
<span class="sd">        ``&#39;samples&#39;``:</span>
<span class="sd">            Calculate metrics for each instance, and find their average (only</span>
<span class="sd">            meaningful for multilabel classification where this differs from</span>
<span class="sd">            :func:`accuracy_score`).</span>

<span class="sd">    warn_for : tuple or set, for internal use</span>
<span class="sd">        This determines which warnings will be made in the case that this</span>
<span class="sd">        function is being used to return only one of its metrics.</span>

<span class="sd">    sample_weight : ndarray, shape (n_samples, )</span>
<span class="sd">        Sample weights.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; from imblearn.metrics import sensitivity_score</span>
<span class="sd">    &gt;&gt;&gt; y_true = [0, 1, 2, 0, 1, 2]</span>
<span class="sd">    &gt;&gt;&gt; y_pred = [0, 2, 1, 0, 0, 1]</span>
<span class="sd">    &gt;&gt;&gt; sensitivity_score(y_true, y_pred, average=&#39;macro&#39;)</span>
<span class="sd">    0.33333333333333331</span>
<span class="sd">    &gt;&gt;&gt; sensitivity_score(y_true, y_pred, average=&#39;micro&#39;)</span>
<span class="sd">    0.33333333333333331</span>
<span class="sd">    &gt;&gt;&gt; sensitivity_score(y_true, y_pred, average=&#39;weighted&#39;)</span>
<span class="sd">    0.33333333333333331</span>
<span class="sd">    &gt;&gt;&gt; sensitivity_score(y_true, y_pred, average=None)</span>
<span class="sd">    array([ 1.,  0.,  0.])</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    specificity : float (if ``average`` = None) or ndarray, \</span>
<span class="sd">        shape (n_unique_labels, )</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">s</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sensitivity_specificity_support</span><span class="p">(</span>
        <span class="n">y_true</span><span class="p">,</span>
        <span class="n">y_pred</span><span class="p">,</span>
        <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
        <span class="n">pos_label</span><span class="o">=</span><span class="n">pos_label</span><span class="p">,</span>
        <span class="n">average</span><span class="o">=</span><span class="n">average</span><span class="p">,</span>
        <span class="n">warn_for</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;sensitivity&#39;</span><span class="p">,</span> <span class="p">),</span>
        <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">s</span></div>


<div class="viewcode-block" id="specificity_score"><a class="viewcode-back" href="../../../generated/imblearn.metrics.specificity_score.html#imblearn.metrics.specificity_score">[docs]</a><span class="k">def</span> <span class="nf">specificity_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span>
                      <span class="n">y_pred</span><span class="p">,</span>
                      <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">pos_label</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                      <span class="n">average</span><span class="o">=</span><span class="s1">&#39;binary&#39;</span><span class="p">,</span>
                      <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the specificity</span>

<span class="sd">    The specificity is the ratio ``tp / (tp + fn)`` where ``tp`` is the number</span>
<span class="sd">    of true positives and ``fn`` the number of false negatives. The specificity</span>
<span class="sd">    is intuitively the ability of the classifier to find all the positive</span>
<span class="sd">    samples.</span>

<span class="sd">    The best value is 1 and the worst value is 0.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : ndarray, shape (n_samples, )</span>
<span class="sd">        Ground truth (correct) target values.</span>

<span class="sd">    y_pred : ndarray, shape (n_samples, )</span>
<span class="sd">        Estimated targets as returned by a classifier.</span>

<span class="sd">    labels : list, optional</span>
<span class="sd">        The set of labels to include when ``average != &#39;binary&#39;``, and their</span>
<span class="sd">        order if ``average is None``. Labels present in the data can be</span>
<span class="sd">        excluded, for example to calculate a multiclass average ignoring a</span>
<span class="sd">        majority negative class, while labels not present in the data will</span>
<span class="sd">        result in 0 components in a macro average.</span>

<span class="sd">    pos_label : str or int, optional (default=1)</span>
<span class="sd">        The class to report if ``average=&#39;binary&#39;`` and the data is binary.</span>
<span class="sd">        If the data are multiclass, this will be ignored;</span>
<span class="sd">        setting ``labels=[pos_label]`` and ``average != &#39;binary&#39;`` will report</span>
<span class="sd">        scores for that label only.</span>

<span class="sd">    average : str or None, optional (default=None)</span>
<span class="sd">        If ``None``, the scores for each class are returned. Otherwise, this</span>
<span class="sd">        determines the type of averaging performed on the data:</span>

<span class="sd">        ``&#39;binary&#39;``:</span>
<span class="sd">            Only report results for the class specified by ``pos_label``.</span>
<span class="sd">            This is applicable only if targets (``y_{true,pred}``) are binary.</span>
<span class="sd">        ``&#39;micro&#39;``:</span>
<span class="sd">            Calculate metrics globally by counting the total true positives,</span>
<span class="sd">            false negatives and false positives.</span>
<span class="sd">        ``&#39;macro&#39;``:</span>
<span class="sd">            Calculate metrics for each label, and find their unweighted</span>
<span class="sd">            mean.  This does not take label imbalance into account.</span>
<span class="sd">        ``&#39;weighted&#39;``:</span>
<span class="sd">            Calculate metrics for each label, and find their average, weighted</span>
<span class="sd">            by support (the number of true instances for each label). This</span>
<span class="sd">            alters &#39;macro&#39; to account for label imbalance; it can result in an</span>
<span class="sd">            F-score that is not between precision and recall.</span>
<span class="sd">        ``&#39;samples&#39;``:</span>
<span class="sd">            Calculate metrics for each instance, and find their average (only</span>
<span class="sd">            meaningful for multilabel classification where this differs from</span>
<span class="sd">            :func:`accuracy_score`).</span>

<span class="sd">    warn_for : tuple or set, for internal use</span>
<span class="sd">        This determines which warnings will be made in the case that this</span>
<span class="sd">        function is being used to return only one of its metrics.</span>

<span class="sd">    sample_weight : ndarray, shape (n_samples, )</span>
<span class="sd">        Sample weights.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; from imblearn.metrics import specificity_score</span>
<span class="sd">    &gt;&gt;&gt; y_true = [0, 1, 2, 0, 1, 2]</span>
<span class="sd">    &gt;&gt;&gt; y_pred = [0, 2, 1, 0, 0, 1]</span>
<span class="sd">    &gt;&gt;&gt; specificity_score(y_true, y_pred, average=&#39;macro&#39;)</span>
<span class="sd">    0.66666666666666663</span>
<span class="sd">    &gt;&gt;&gt; specificity_score(y_true, y_pred, average=&#39;micro&#39;)</span>
<span class="sd">    0.66666666666666663</span>
<span class="sd">    &gt;&gt;&gt; specificity_score(y_true, y_pred, average=&#39;weighted&#39;)</span>
<span class="sd">    0.66666666666666663</span>
<span class="sd">    &gt;&gt;&gt; specificity_score(y_true, y_pred, average=None)</span>
<span class="sd">    array([ 0.75,  0.5 ,  0.75])</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    specificity : float (if ``average`` = None) or ndarray, \</span>
<span class="sd">        shape (n_unique_labels, )</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sensitivity_specificity_support</span><span class="p">(</span>
        <span class="n">y_true</span><span class="p">,</span>
        <span class="n">y_pred</span><span class="p">,</span>
        <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
        <span class="n">pos_label</span><span class="o">=</span><span class="n">pos_label</span><span class="p">,</span>
        <span class="n">average</span><span class="o">=</span><span class="n">average</span><span class="p">,</span>
        <span class="n">warn_for</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;specificity&#39;</span><span class="p">,</span> <span class="p">),</span>
        <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">s</span></div>


<div class="viewcode-block" id="geometric_mean_score"><a class="viewcode-back" href="../../../generated/imblearn.metrics.geometric_mean_score.html#imblearn.metrics.geometric_mean_score">[docs]</a><span class="k">def</span> <span class="nf">geometric_mean_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span>
                         <span class="n">y_pred</span><span class="p">,</span>
                         <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">pos_label</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                         <span class="n">average</span><span class="o">=</span><span class="s1">&#39;multiclass&#39;</span><span class="p">,</span>
                         <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">correction</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the geometric mean</span>

<span class="sd">    The geometric mean (G-mean) is the root of the product of class-wise</span>
<span class="sd">    sensitivity. This measure tries to maximize the accuracy on each of the</span>
<span class="sd">    classes while keeping these accuracies balanced. For binary classification</span>
<span class="sd">    G-mean is the squared root of the product of the sensitivity</span>
<span class="sd">    and specificity. For multi-class problems it is a higher root of the</span>
<span class="sd">    product of sensitivity for each class.</span>

<span class="sd">    For compatibility with other imbalance performance measures, G-mean can be</span>
<span class="sd">    calculated for each class separately on a one-vs-rest basis when</span>
<span class="sd">    ``average != &#39;multiclass&#39;``.</span>

<span class="sd">    The best value is 1 and the worst value is 0. Traditionally if at least one</span>
<span class="sd">    class is unrecognized by the classifier, G-mean resolves to zero. To</span>
<span class="sd">    alleviate this property, for highly multi-class the sensitivity of</span>
<span class="sd">    unrecognized classes can be &quot;corrected&quot; to be a user specified value</span>
<span class="sd">    (instead of zero). This option works only if ``average == &#39;multiclass&#39;``.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : ndarray, shape (n_samples, )</span>
<span class="sd">        Ground truth (correct) target values.</span>

<span class="sd">    y_pred : ndarray, shape (n_samples, )</span>
<span class="sd">        Estimated targets as returned by a classifier.</span>

<span class="sd">    labels : list, optional</span>
<span class="sd">        The set of labels to include when ``average != &#39;binary&#39;``, and their</span>
<span class="sd">        order if ``average is None``. Labels present in the data can be</span>
<span class="sd">        excluded, for example to calculate a multiclass average ignoring a</span>
<span class="sd">        majority negative class, while labels not present in the data will</span>
<span class="sd">        result in 0 components in a macro average.</span>

<span class="sd">    pos_label : str or int, optional (default=1)</span>
<span class="sd">        The class to report if ``average=&#39;binary&#39;`` and the data is binary.</span>
<span class="sd">        If the data are multiclass, this will be ignored;</span>
<span class="sd">        setting ``labels=[pos_label]`` and ``average != &#39;binary&#39;`` will report</span>
<span class="sd">        scores for that label only.</span>

<span class="sd">    average : str or None, optional (default=``&#39;multiclass&#39;``)</span>
<span class="sd">        If ``None``, the scores for each class are returned. Otherwise, this</span>
<span class="sd">        determines the type of averaging performed on the data:</span>

<span class="sd">        ``&#39;binary&#39;``:</span>
<span class="sd">            Only report results for the class specified by ``pos_label``.</span>
<span class="sd">            This is applicable only if targets (``y_{true,pred}``) are binary.</span>
<span class="sd">        ``&#39;micro&#39;``:</span>
<span class="sd">            Calculate metrics globally by counting the total true positives,</span>
<span class="sd">            false negatives and false positives.</span>
<span class="sd">        ``&#39;macro&#39;``:</span>
<span class="sd">            Calculate metrics for each label, and find their unweighted</span>
<span class="sd">            mean.  This does not take label imbalance into account.</span>
<span class="sd">        ``&#39;weighted&#39;``:</span>
<span class="sd">            Calculate metrics for each label, and find their average, weighted</span>
<span class="sd">            by support (the number of true instances for each label). This</span>
<span class="sd">            alters &#39;macro&#39; to account for label imbalance; it can result in an</span>
<span class="sd">            F-score that is not between precision and recall.</span>
<span class="sd">        ``&#39;samples&#39;``:</span>
<span class="sd">            Calculate metrics for each instance, and find their average (only</span>
<span class="sd">            meaningful for multilabel classification where this differs from</span>
<span class="sd">            :func:`accuracy_score`).</span>

<span class="sd">    sample_weight : ndarray, shape (n_samples, )</span>
<span class="sd">        Sample weights.</span>

<span class="sd">    correction: float, optional (default=0.0)</span>
<span class="sd">        Substitutes sensitivity of unrecognized classes from zero to a given</span>
<span class="sd">        value.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    geometric_mean : float</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from imblearn.metrics import geometric_mean_score</span>
<span class="sd">    &gt;&gt;&gt; y_true = [0, 1, 2, 0, 1, 2]</span>
<span class="sd">    &gt;&gt;&gt; y_pred = [0, 2, 1, 0, 0, 1]</span>
<span class="sd">    &gt;&gt;&gt; geometric_mean_score(y_true, y_pred)</span>
<span class="sd">    0.0</span>
<span class="sd">    &gt;&gt;&gt; geometric_mean_score(y_true, y_pred, correction=0.001)</span>
<span class="sd">    0.010000000000000004</span>
<span class="sd">    &gt;&gt;&gt; geometric_mean_score(y_true, y_pred, average=&#39;macro&#39;)</span>
<span class="sd">    0.47140452079103168</span>
<span class="sd">    &gt;&gt;&gt; geometric_mean_score(y_true, y_pred, average=&#39;micro&#39;)</span>
<span class="sd">    0.47140452079103168</span>
<span class="sd">    &gt;&gt;&gt; geometric_mean_score(y_true, y_pred, average=&#39;weighted&#39;)</span>
<span class="sd">    0.47140452079103168</span>
<span class="sd">    &gt;&gt;&gt; geometric_mean_score(y_true, y_pred, average=None)</span>
<span class="sd">    array([ 0.8660254,  0.       ,  0.       ])</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] Kubat, M. and Matwin, S. &quot;Addressing the curse of</span>
<span class="sd">       imbalanced training sets: one-sided selection&quot; ICML (1997)</span>

<span class="sd">    .. [2] Barandela, R., Snchez, J. S., Garca, V., &amp; Rangel, E. &quot;Strategies</span>
<span class="sd">       for learning in class imbalance problems&quot;, Pattern Recognition,</span>
<span class="sd">       36(3), (2003), pp 849-851.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">average</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">average</span> <span class="o">!=</span> <span class="s1">&#39;multiclass&#39;</span><span class="p">:</span>
        <span class="n">sen</span><span class="p">,</span> <span class="n">spe</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sensitivity_specificity_support</span><span class="p">(</span>
            <span class="n">y_true</span><span class="p">,</span>
            <span class="n">y_pred</span><span class="p">,</span>
            <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
            <span class="n">pos_label</span><span class="o">=</span><span class="n">pos_label</span><span class="p">,</span>
            <span class="n">average</span><span class="o">=</span><span class="n">average</span><span class="p">,</span>
            <span class="n">warn_for</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;specificity&#39;</span><span class="p">,</span> <span class="s1">&#39;specificity&#39;</span><span class="p">),</span>
            <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>

        <span class="n">LOGGER</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s1">&#39;The sensitivity and specificity are : </span><span class="si">%s</span><span class="s1"> - </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span>
                     <span class="p">(</span><span class="n">sen</span><span class="p">,</span> <span class="n">spe</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sen</span> <span class="o">*</span> <span class="n">spe</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">present_labels</span> <span class="o">=</span> <span class="n">unique_labels</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">present_labels</span>
            <span class="n">n_labels</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">n_labels</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">labels</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">setdiff1d</span><span class="p">(</span><span class="n">present_labels</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span>
                                                     <span class="n">assume_unique</span><span class="o">=</span><span class="kc">True</span><span class="p">)])</span>

        <span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
        <span class="n">le</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
        <span class="n">y_true</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">sorted_labels</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">classes_</span>

        <span class="c1"># labels are now from 0 to len(labels) - 1 -&gt; use bincount</span>
        <span class="n">tp</span> <span class="o">=</span> <span class="n">y_true</span> <span class="o">==</span> <span class="n">y_pred</span>
        <span class="n">tp_bins</span> <span class="o">=</span> <span class="n">y_true</span><span class="p">[</span><span class="n">tp</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tp_bins_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">)[</span><span class="n">tp</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tp_bins_weights</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tp_bins</span><span class="p">):</span>
            <span class="n">tp_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">tp_bins</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">tp_bins_weights</span><span class="p">,</span>
                                 <span class="n">minlength</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Pathological case</span>
            <span class="n">true_sum</span> <span class="o">=</span> <span class="n">tp_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">):</span>
            <span class="n">true_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                                   <span class="n">minlength</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span>

        <span class="c1"># Retain only selected labels</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">searchsorted</span><span class="p">(</span><span class="n">sorted_labels</span><span class="p">,</span> <span class="n">labels</span><span class="p">[:</span><span class="n">n_labels</span><span class="p">])</span>
        <span class="n">tp_sum</span> <span class="o">=</span> <span class="n">tp_sum</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
        <span class="n">true_sum</span> <span class="o">=</span> <span class="n">true_sum</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>

        <span class="n">recall</span> <span class="o">=</span> <span class="n">_prf_divide</span><span class="p">(</span><span class="n">tp_sum</span><span class="p">,</span> <span class="n">true_sum</span><span class="p">,</span> <span class="s2">&quot;recall&quot;</span><span class="p">,</span> <span class="s2">&quot;true&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
                             <span class="s2">&quot;recall&quot;</span><span class="p">)</span>
        <span class="n">recall</span><span class="p">[</span><span class="n">recall</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">correction</span>

        <span class="k">return</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">mstats</span><span class="o">.</span><span class="n">gmean</span><span class="p">(</span><span class="n">recall</span><span class="p">)</span></div>


<div class="viewcode-block" id="make_index_balanced_accuracy"><a class="viewcode-back" href="../../../generated/imblearn.metrics.make_index_balanced_accuracy.html#imblearn.metrics.make_index_balanced_accuracy">[docs]</a><span class="k">def</span> <span class="nf">make_index_balanced_accuracy</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Balance any scoring function using the index balanced accuracy</span>

<span class="sd">    This factory function wraps scoring function to express it as the</span>
<span class="sd">    index balanced accuracy (IBA). You need to use this function to</span>
<span class="sd">    decorate any scoring function.</span>

<span class="sd">    Only metrics requiring ``y_pred`` can be corrected with the index</span>
<span class="sd">    balanced accuracy. ``y_score`` cannot be used since the dominance</span>
<span class="sd">    cannot be computed.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    alpha : float, optional (default=0.1)</span>
<span class="sd">        Weighting factor.</span>

<span class="sd">    squared : bool, optional (default=True)</span>
<span class="sd">        If ``squared`` is True, then the metric computed will be squared</span>
<span class="sd">        before to be weighted.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    iba_scoring_func : callable,</span>
<span class="sd">        Returns the scoring metric decorated which will automatically compute</span>
<span class="sd">        the index balanced accuracy.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from imblearn.metrics import geometric_mean_score as gmean</span>
<span class="sd">    &gt;&gt;&gt; from imblearn.metrics import make_index_balanced_accuracy as iba</span>
<span class="sd">    &gt;&gt;&gt; gmean = iba(alpha=0.1, squared=True)(gmean)</span>
<span class="sd">    &gt;&gt;&gt; y_true = [1, 0, 0, 1, 0, 1]</span>
<span class="sd">    &gt;&gt;&gt; y_pred = [0, 0, 1, 1, 0, 1]</span>
<span class="sd">    &gt;&gt;&gt; print(gmean(y_true, y_pred, average=None))</span>
<span class="sd">    [ 0.44444444  0.44444444]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">decorate</span><span class="p">(</span><span class="n">scoring_func</span><span class="p">):</span>
        <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">scoring_func</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">compute_score</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="c1"># Create the list of tags</span>
            <span class="n">tags_scoring_func</span> <span class="o">=</span> <span class="n">getcallargs</span><span class="p">(</span><span class="n">scoring_func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="c1"># check that the scoring function does not need a score</span>
            <span class="c1"># and only a prediction</span>
            <span class="k">if</span> <span class="p">(</span><span class="s1">&#39;y_score&#39;</span> <span class="ow">in</span> <span class="n">tags_scoring_func</span> <span class="ow">or</span>
                <span class="s1">&#39;y_prob&#39;</span> <span class="ow">in</span> <span class="n">tags_scoring_func</span> <span class="ow">or</span>
                    <span class="s1">&#39;y2&#39;</span> <span class="ow">in</span> <span class="n">tags_scoring_func</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;The function </span><span class="si">{}</span><span class="s1"> has an unsupported&#39;</span>
                                     <span class="s1">&#39; attribute. Metric with`y_pred` are the&#39;</span>
                                     <span class="s1">&#39; only supported metrics is the only&#39;</span>
                                     <span class="s1">&#39; supported.&#39;</span><span class="p">)</span>
            <span class="c1"># Compute the score from the scoring function</span>
            <span class="n">_score</span> <span class="o">=</span> <span class="n">scoring_func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="c1"># Square if desired</span>
            <span class="k">if</span> <span class="n">squared</span><span class="p">:</span>
                <span class="n">_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">_score</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="c1"># Get the signature of the sens/spec function</span>
            <span class="n">sens_spec_sig</span> <span class="o">=</span> <span class="n">signature</span><span class="p">(</span><span class="n">sensitivity_specificity_support</span><span class="p">)</span>
            <span class="c1"># We need to extract from kwargs only the one needed by the</span>
            <span class="c1"># specificity and specificity</span>
            <span class="n">params_sens_spec</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">sens_spec_sig</span><span class="o">.</span><span class="n">_parameters</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
            <span class="c1"># Make the intersection between the parameters</span>
            <span class="n">sel_params</span> <span class="o">=</span> <span class="n">params_sens_spec</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span>
                <span class="nb">set</span><span class="p">(</span><span class="n">tags_scoring_func</span><span class="p">))</span>
            <span class="c1"># Create a sub dictionary</span>
            <span class="n">tags_scoring_func</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="n">tags_scoring_func</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
                                     <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">sel_params</span><span class="p">)</span>
            <span class="c1"># Check if the metric is the geometric mean</span>
            <span class="k">if</span> <span class="n">scoring_func</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;geometric_mean_score&#39;</span><span class="p">:</span>
                <span class="k">if</span> <span class="s1">&#39;average&#39;</span> <span class="ow">in</span> <span class="n">tags_scoring_func</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">tags_scoring_func</span><span class="p">[</span><span class="s1">&#39;average&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;multiclass&#39;</span><span class="p">:</span>
                        <span class="n">tags_scoring_func</span><span class="p">[</span><span class="s1">&#39;average&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;macro&#39;</span>
            <span class="c1"># We do not support multilabel so the only average supported</span>
            <span class="c1"># is binary</span>
            <span class="k">elif</span> <span class="p">(</span><span class="n">scoring_func</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;accuracy_score&#39;</span> <span class="ow">or</span>
                  <span class="n">scoring_func</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;jaccard_similarity_score&#39;</span><span class="p">):</span>
                <span class="n">tags_scoring_func</span><span class="p">[</span><span class="s1">&#39;average&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;binary&#39;</span>
            <span class="c1"># Create the list of parameters through signature binding</span>
            <span class="n">tags_sens_spec</span> <span class="o">=</span> <span class="n">sens_spec_sig</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span>
                <span class="o">**</span><span class="n">tags_scoring_func</span><span class="p">)</span>
            <span class="c1"># Call the sens/spec function</span>
            <span class="n">sen</span><span class="p">,</span> <span class="n">spe</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sensitivity_specificity_support</span><span class="p">(</span>
                <span class="o">*</span><span class="n">tags_sens_spec</span><span class="o">.</span><span class="n">args</span><span class="p">,</span>
                <span class="o">**</span><span class="n">tags_sens_spec</span><span class="o">.</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="c1"># Compute the dominance</span>
            <span class="n">dom</span> <span class="o">=</span> <span class="n">sen</span> <span class="o">-</span> <span class="n">spe</span>
            <span class="k">return</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dom</span><span class="p">)</span> <span class="o">*</span> <span class="n">_score</span>

        <span class="k">return</span> <span class="n">compute_score</span>

    <span class="k">return</span> <span class="n">decorate</span></div>


<div class="viewcode-block" id="classification_report_imbalanced"><a class="viewcode-back" href="../../../generated/imblearn.metrics.classification_report_imbalanced.html#imblearn.metrics.classification_report_imbalanced">[docs]</a><span class="k">def</span> <span class="nf">classification_report_imbalanced</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span>
                                     <span class="n">y_pred</span><span class="p">,</span>
                                     <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">target_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">digits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                     <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Build a classification report based on metrics used with imbalanced</span>
<span class="sd">    dataset</span>

<span class="sd">    Specific metrics have been proposed to evaluate the classification</span>
<span class="sd">    performed on imbalanced dataset. This report compiles the</span>
<span class="sd">    state-of-the-art metrics: precision/recall/specificity, geometric</span>
<span class="sd">    mean, and index balanced accuracy of the</span>
<span class="sd">    geometric mean.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : ndarray, shape (n_samples, )</span>
<span class="sd">        Ground truth (correct) target values.</span>

<span class="sd">    y_pred : ndarray, shape (n_samples, )</span>
<span class="sd">        Estimated targets as returned by a classifier.</span>

<span class="sd">    labels : list, optional</span>
<span class="sd">        The set of labels to include when ``average != &#39;binary&#39;``, and their</span>
<span class="sd">        order if ``average is None``. Labels present in the data can be</span>
<span class="sd">        excluded, for example to calculate a multiclass average ignoring a</span>
<span class="sd">        majority negative class, while labels not present in the data will</span>
<span class="sd">        result in 0 components in a macro average.</span>

<span class="sd">    target_names : list of strings, optional</span>
<span class="sd">        Optional display names matching the labels (same order).</span>

<span class="sd">    sample_weight : ndarray, shape (n_samples, )</span>
<span class="sd">        Sample weights.</span>

<span class="sd">    digits : int, optional (default=2)</span>
<span class="sd">        Number of digits for formatting output floating point values</span>

<span class="sd">    alpha : float, optional (default=0.1)</span>
<span class="sd">        Weighting factor.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    report : string</span>
<span class="sd">        Text summary of the precision, recall, specificity, geometric mean,</span>
<span class="sd">        and index balanced accuracy.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; from imblearn.metrics import classification_report_imbalanced</span>
<span class="sd">    &gt;&gt;&gt; y_true = [0, 1, 2, 2, 2]</span>
<span class="sd">    &gt;&gt;&gt; y_pred = [0, 0, 2, 2, 1] # doctest : +NORMALIZE_WHITESPACE</span>
<span class="sd">    &gt;&gt;&gt; target_names = [&#39;class 0&#39;, &#39;class 1&#39;, \</span>
<span class="sd">    &#39;class 2&#39;] # doctest : +NORMALIZE_WHITESPACE</span>
<span class="sd">    &gt;&gt;&gt; print(classification_report_imbalanced(y_true, y_pred, \</span>
<span class="sd">    target_names=target_names))</span>
<span class="sd">                       pre       rec       spe        f1       geo       iba\</span>
<span class="sd">       sup</span>
<span class="sd">    &lt;BLANKLINE&gt;</span>
<span class="sd">        class 0       0.50      1.00      0.75      0.67      0.71      0.48\</span>
<span class="sd">         1</span>
<span class="sd">        class 1       0.00      0.00      0.75      0.00      0.00      0.00\</span>
<span class="sd">         1</span>
<span class="sd">        class 2       1.00      0.67      1.00      0.80      0.82      0.69\</span>
<span class="sd">         3</span>
<span class="sd">    &lt;BLANKLINE&gt;</span>
<span class="sd">    avg / total       0.70      0.60      0.90      0.61      0.63      0.51\</span>
<span class="sd">         5</span>
<span class="sd">    &lt;BLANKLINE&gt;</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">unique_labels</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

    <span class="n">last_line_heading</span> <span class="o">=</span> <span class="s1">&#39;avg / total&#39;</span>

    <span class="k">if</span> <span class="n">target_names</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">target_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">l</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]</span>
    <span class="n">name_width</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cn</span><span class="p">)</span> <span class="k">for</span> <span class="n">cn</span> <span class="ow">in</span> <span class="n">target_names</span><span class="p">)</span>
    <span class="n">width</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">name_width</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">last_line_heading</span><span class="p">),</span> <span class="n">digits</span><span class="p">)</span>

    <span class="n">headers</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;pre&quot;</span><span class="p">,</span> <span class="s2">&quot;rec&quot;</span><span class="p">,</span> <span class="s2">&quot;spe&quot;</span><span class="p">,</span> <span class="s2">&quot;f1&quot;</span><span class="p">,</span> <span class="s2">&quot;geo&quot;</span><span class="p">,</span> <span class="s2">&quot;iba&quot;</span><span class="p">,</span> <span class="s2">&quot;sup&quot;</span><span class="p">]</span>
    <span class="n">fmt</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">%%</span><span class="s1"> </span><span class="si">%d</span><span class="s1">s&#39;</span> <span class="o">%</span> <span class="n">width</span>  <span class="c1"># first column: class name</span>
    <span class="n">fmt</span> <span class="o">+=</span> <span class="s1">&#39;  &#39;</span>
    <span class="n">fmt</span> <span class="o">+=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s1">&#39;</span><span class="si">% 9s</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">headers</span><span class="p">])</span>
    <span class="n">fmt</span> <span class="o">+=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>

    <span class="n">headers</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">headers</span>
    <span class="n">report</span> <span class="o">=</span> <span class="n">fmt</span> <span class="o">%</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">headers</span><span class="p">)</span>
    <span class="n">report</span> <span class="o">+=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>

    <span class="c1"># Compute the different metrics</span>
    <span class="c1"># Precision/recall/f1</span>
    <span class="n">precision</span><span class="p">,</span> <span class="n">recall</span><span class="p">,</span> <span class="n">f1</span><span class="p">,</span> <span class="n">support</span> <span class="o">=</span> <span class="n">precision_recall_fscore_support</span><span class="p">(</span>
        <span class="n">y_true</span><span class="p">,</span>
        <span class="n">y_pred</span><span class="p">,</span>
        <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
        <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
    <span class="c1"># Specificity</span>
    <span class="n">specificity</span> <span class="o">=</span> <span class="n">specificity_score</span><span class="p">(</span>
        <span class="n">y_true</span><span class="p">,</span>
        <span class="n">y_pred</span><span class="p">,</span>
        <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
        <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
    <span class="c1"># Geometric mean</span>
    <span class="n">geo_mean</span> <span class="o">=</span> <span class="n">geometric_mean_score</span><span class="p">(</span>
        <span class="n">y_pred</span><span class="p">,</span>
        <span class="n">y_true</span><span class="p">,</span>
        <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
        <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
    <span class="c1"># Index balanced accuracy</span>
    <span class="n">iba_gmean</span> <span class="o">=</span> <span class="n">make_index_balanced_accuracy</span><span class="p">(</span>
        <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">geometric_mean_score</span><span class="p">)</span>
    <span class="n">iba</span> <span class="o">=</span> <span class="n">iba_gmean</span><span class="p">(</span>
        <span class="n">y_pred</span><span class="p">,</span>
        <span class="n">y_true</span><span class="p">,</span>
        <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
        <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">labels</span><span class="p">):</span>
        <span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="n">target_names</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
        <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">(</span><span class="n">precision</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">recall</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">specificity</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">f1</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">geo_mean</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                  <span class="n">iba</span><span class="p">[</span><span class="n">i</span><span class="p">]):</span>
            <span class="n">values</span> <span class="o">+=</span> <span class="p">[</span><span class="s2">&quot;{0:0.</span><span class="si">{1}</span><span class="s2">f}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">digits</span><span class="p">)]</span>
        <span class="n">values</span> <span class="o">+=</span> <span class="p">[</span><span class="s2">&quot;</span><span class="si">{0}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">support</span><span class="p">[</span><span class="n">i</span><span class="p">])]</span>
        <span class="n">report</span> <span class="o">+=</span> <span class="n">fmt</span> <span class="o">%</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>

    <span class="n">report</span> <span class="o">+=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>

    <span class="c1"># compute averages</span>
    <span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="n">last_line_heading</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span>
            <span class="n">precision</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">support</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span>
                <span class="n">recall</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">support</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span>
                    <span class="n">specificity</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">support</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span>
                        <span class="n">f1</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">support</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span>
                            <span class="n">geo_mean</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">support</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span>
                                <span class="n">iba</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">support</span><span class="p">)):</span>
        <span class="n">values</span> <span class="o">+=</span> <span class="p">[</span><span class="s2">&quot;{0:0.</span><span class="si">{1}</span><span class="s2">f}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">digits</span><span class="p">)]</span>
    <span class="n">values</span> <span class="o">+=</span> <span class="p">[</span><span class="s1">&#39;</span><span class="si">{0}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">support</span><span class="p">))]</span>
    <span class="n">report</span> <span class="o">+=</span> <span class="n">fmt</span> <span class="o">%</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">report</span></div>
</pre></div>

           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, G. Lemaitre, F. Nogueira, D. Oliveira, C. Aridas.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../',
            VERSION:'0.3.0.dev0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../../_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>